{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# __Kernel-based Quantum Machine Learning__\n",
    "\n",
    "November 14, 2023\n",
    "\n",
    "<br>\n",
    "\n",
    "__Computación Cuántica y Aprendizaje Máquina__\n",
    "\n",
    "__Máster en Ciencia y Tecnologías de Información Cuántica (MQIST)__\n",
    "\n",
    "<br>\n",
    "\n",
    "Diego Alvarez-Estevez, PhD.\n",
    "\n",
    "_Centro de Investigación en Tecnologías de la Información y las Comunicaciones (CITIC)_\n",
    "\n",
    "_Universidade da Coruña_\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The general task of machine learning is to find and study patterns in data. For many datasets, the datapoints are better understood in a higher dimensional feature space. This is the fundamental principle behind a series of machine learning algorithms known as *kernel methods*.\n",
    "\n",
    "In this notebook, you will learn how to define quantum kernels using `qiskit-machine-learning` and how these can be plugged into different algorithms to solve classification and clustering problems.\n",
    "\n",
    "All examples used in this tutorial are based on this reference paper: [_Supervised learning with quantum enhanced feature spaces_](https://arxiv.org/pdf/1804.11326.pdf).\n",
    "\n",
    "The content is structured as follows:\n",
    "\n",
    "1. [Introduction](#1.-Introduction)\n",
    "2. [Classification](#2.-Classification)\n",
    "3. [Kernel Principal Components Analysis](#3.-Kernel-Principal-Component-Analysis)\n",
    "4. [Conclusions](#4.-Conclusion)\n",
    "5. [Exercises](#5.-Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1. Kernel Methods for Machine Learning\n",
    "\n",
    "Kernel methods are a collection of pattern analysis algorithms that use kernel functions to operate in a high-dimensional feature space. The best-known application of kernel methods is in **Support Vector Machines (SVMs)**, supervised learning algorithms commonly used for classification tasks. The main goal of SVMs is to find decision boundaries to separate a given set of data points into classes. When these data spaces are not linearly separable, SVMs can benefit from the use of kernels to find these boundaries.\n",
    "\n",
    "Formally, decision boundaries are hyperplanes in a high dimensional space. The kernel function implicitly maps input data into this higher dimensional space, where it can be easier to solve the initial problem. In other words, kernels may allow data distributions that were originally non-linearly separable to become a linearly separable problem. This is an effect known as the \"kernel trick\".\n",
    "\n",
    "There are use-cases for kernel-based unsupervised algorithms too, for example, in the context of clustering or dimensionality reduction.\n",
    "\n",
    "### 1.2. Kernel Functions\n",
    "\n",
    "Mathematically, kernel functions follow:\n",
    "\n",
    "$k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle$\n",
    "\n",
    "where \n",
    "* $k$ is the kernel function\n",
    "* $\\vec{x}_i, \\vec{x}_j$ are $d$-dimensional inputs\n",
    "* $f$ is a map from $d$-dimension to $d_f$-dimension space (usually $d_f >> d$) and \n",
    "* $\\langle a,b \\rangle$ denotes the inner product\n",
    "\n",
    "When considering finite data, a kernel function can be represented as a matrix: \n",
    "\n",
    "$K_{ij} = k(\\vec{x}_i,\\vec{x}_j)$.\n",
    "\n",
    "### 1.3. Quantum Kernels\n",
    "\n",
    "The main idea behind quantum kernel machine learning is to leverage quantum feature maps to perform the kernel trick. In this case, the quantum kernel is created by mapping a classical feature vector $\\vec{x}$ to a Hilbert space using a quantum feature map $\\phi(\\vec{x})$. Mathematically:\n",
    "\n",
    "$K_{ij} = \\left| \\langle \\phi(\\vec{x}_i)| \\phi(\\vec{x}_j) \\rangle \\right|^{2}$\n",
    "\n",
    "where \n",
    "* $K_{ij}$ is the kernel matrix\n",
    "* $\\vec{x}_i, \\vec{x}_j$ are $d$-dimensional inputs\n",
    "* $\\phi(\\vec{x})$ is the quantum feature map\n",
    "* $\\left| \\langle a|b \\rangle \\right|^{2}$ denotes the overlap of two quantum states $a$ and $b$\n",
    "\n",
    "Quantum kernels can be plugged into common classical kernel learning algorithms such as SVMs or kPCA algorithms, as you will see in the examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "\n",
    "This section illustrates a quantum kernel classification workflow using [qiskit-machine-learning](https://github.com/qiskit-community/qiskit-machine-learning).\n",
    "\n",
    "### 2.0. Basic setup:\n",
    "\n",
    "Installation of all required Python libraries and most of related dependencies for running this notebook should be handled automatically with installation of the ```qiskit-machine-learning``` package using the following command:\n",
    "\n",
    "**_pip install qiskit-machine-learning_**\n",
    "\n",
    "For visualization rutines and better integration with Jupyter notebooks it is recommended that you also run the following command:\n",
    "\n",
    "**_pip install qiskit[visualization]_**\n",
    "\n",
    "In particular, check that you have installed a version of the Qiskit package (at least version >= 0.44), ```qiskit-machine-learning``` package (version >= 0.6.0) and ```qiskit-algorithms``` (version >= 0.20).\n",
    "\n",
    "Otherwise proceed to install the required packages individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version checking\n",
    "import qiskit.tools.jupyter\n",
    "import qiskit_machine_learning\n",
    "import qiskit_algorithms\n",
    "\n",
    "%qiskit_version_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, before proceeding, we set up the global seed to ensure reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_algorithms.utils import algorithm_globals \n",
    "\n",
    "algorithm_globals.random_seed = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Defining the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use the _ad hoc dataset_ as described in the reference [paper](https://arxiv.org/pdf/1804.11326.pdf). \n",
    "\n",
    "We can define the dataset dimension and get our train and test subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.datasets import ad_hoc_data\n",
    "\n",
    "adhoc_dimension = 2\n",
    "train_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n",
    "    training_size=20, # for each class, thus 40 in total\n",
    "    test_size=5,\n",
    "    n=adhoc_dimension,\n",
    "    gap=0.3,\n",
    "    #plot_data=True,\n",
    "    plot_data=False,\n",
    "    one_hot=False,\n",
    "    include_sample_total=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is two-dimensional, the two features are represented by the $x$ and $y$ coordinates, and it has two class labels: _A_ and _B_. We can plot it and see what the distribution looks like. We define utility functions to plot the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_features(ax, features, labels, class_label, marker, face, edge, label):\n",
    "    # A train plot\n",
    "    ax.scatter(\n",
    "        # x coordinate of labels where class is class_label\n",
    "        features[np.where(labels[:] == class_label), 0],\n",
    "        # y coordinate of labels where class is class_label\n",
    "        features[np.where(labels[:] == class_label), 1],\n",
    "        marker=marker,\n",
    "        facecolors=face,\n",
    "        edgecolors=edge,\n",
    "        label=label,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_dataset(train_features, train_labels, test_features, test_labels, adhoc_total):\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.ylim(0, 2 * np.pi)\n",
    "    plt.xlim(0, 2 * np.pi)\n",
    "    plt.imshow(\n",
    "        np.asmatrix(adhoc_total).T,\n",
    "        interpolation=\"nearest\",\n",
    "        origin=\"lower\",\n",
    "        cmap=\"RdBu\",\n",
    "        extent=[0, 2 * np.pi, 0, 2 * np.pi],\n",
    "    )\n",
    "\n",
    "    # A train plot\n",
    "    plot_features(plt, train_features, train_labels, 0, \"s\", \"w\", \"b\", \"A train\")\n",
    "\n",
    "    # B train plot\n",
    "    plot_features(plt, train_features, train_labels, 1, \"o\", \"w\", \"r\", \"B train\")\n",
    "\n",
    "    # A test plot\n",
    "    plot_features(plt, test_features, test_labels, 0, \"s\", \"b\", \"w\", \"A test\")\n",
    "\n",
    "    # B test plot\n",
    "    plot_features(plt, test_features, test_labels, 1, \"o\", \"r\", \"w\", \"B test\")\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "    plt.title(\"Ad hoc dataset\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually plot the dataset for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "plot_dataset(train_features, train_labels, test_features, test_labels, adhoc_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Defining the quantum kernel\n",
    "\n",
    "The next step is to create a quantum kernel instance that will help classify this data. \n",
    "\n",
    "We use the [FidelityQuantumKernel](https://qiskit.org/ecosystem/machine-learning/stubs/qiskit_machine_learning.kernels.FidelityQuantumKernel.html) class, and pass two input arguments to its constructor: \n",
    "\n",
    "1. `feature_map`: in this case, a two-qubit [ZZFeatureMap](https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html).\n",
    "\n",
    "2. `fidelity`: in this case, the [ComputeUncompute](https://qiskit.org/documentation/stubs/qiskit.algorithms.state_fidelities.ComputeUncompute.html) fidelity subroutine that leverages the [Sampler](https://qiskit.org/documentation/stubs/qiskit.primitives.Sampler.html) primitive.\n",
    "\n",
    "**NOTE:** If you don't pass a `Sampler` or `Fidelity` instance, then the instances of the reference `Sampler` and `ComputeUncompute` classes (found in `qiskit.primitives`) will be created by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "# Using Qiskit Terra's Sampler reference, use Aer / QiskitRuntime for improved performance\n",
    "# More information: https://medium.com/qiskit/introducing-qiskit-machine-learning-0-6-25186b57bf97\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_algorithms.state_fidelities import ComputeUncompute\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "\n",
    "adhoc_feature_map = ZZFeatureMap(feature_dimension=adhoc_dimension, reps=2, entanglement=\"linear\")\n",
    "\n",
    "sampler = Sampler()\n",
    "\n",
    "fidelity = ComputeUncompute(sampler=sampler)\n",
    "\n",
    "adhoc_kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=adhoc_feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Classification with SVC\n",
    "The quantum kernel can now be plugged into classical kernel methods, such as the [SVC](https://scikit-learn.org/stable/modules/svm.html) algorithm from `scikit-learn`. This algorithm allows us to define a [custom kernel](https://scikit-learn.org/stable/modules/svm.html#custom-kernels) in two ways:\n",
    "\n",
    "1. by providing the kernel as a **callable function**\n",
    "2. by precomputing the **kernel matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel as a callable function\n",
    "\n",
    "We define a SVC model and directly pass the `evaluate` function of the quantum kernel as a callable. Once the model is created, we train it by calling the `fit` method on the training dataset and evaluate the model for accuracy with `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "adhoc_svc = SVC(kernel=adhoc_kernel.evaluate)\n",
    "\n",
    "adhoc_svc.fit(train_features, train_labels)\n",
    "\n",
    "adhoc_score_callable_function = adhoc_svc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Callable kernel classification test score: {adhoc_score_callable_function}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precomputed kernel matrix\n",
    "\n",
    "Instead of passing a function of the quantum kernel as a callable, we can also precompute training and testing kernel matrices before passing them to the `scikit-learn` `SVC` algorithm. \n",
    "\n",
    "To extract the train and test matrices, we can call `evaluate` on the previously defined kernel and visualize them graphically as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhoc_matrix_train = adhoc_kernel.evaluate(x_vec=train_features)\n",
    "adhoc_matrix_test = adhoc_kernel.evaluate(x_vec=test_features, y_vec=train_features)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(\n",
    "    np.asmatrix(adhoc_matrix_train), interpolation=\"nearest\", origin=\"upper\", cmap=\"Blues\"\n",
    ")\n",
    "axs[0].set_title(\"Ad hoc training kernel matrix\")\n",
    "\n",
    "axs[1].imshow(np.asmatrix(adhoc_matrix_test), interpolation=\"nearest\", origin=\"upper\", cmap=\"Reds\")\n",
    "axs[1].set_title(\"Ad hoc testing kernel matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use these matrices, we set the `kernel` parameter of a new `SVC` instance to `\"precomputed\"`. We train the classifier by calling `fit` with the training matrix and training dataset. Once the model is trained, we evaluate it using the test matrix on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhoc_svc = SVC(kernel=\"precomputed\")\n",
    "\n",
    "adhoc_svc.fit(adhoc_matrix_train, train_labels)\n",
    "\n",
    "adhoc_score_precomputed_kernel = adhoc_svc.score(adhoc_matrix_test, test_labels)\n",
    "\n",
    "print(f\"Precomputed kernel classification test score: {adhoc_score_precomputed_kernel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Classification with QSVC\n",
    "\n",
    "`QSVC` is an alternative training algorithm provided by `qiskit-machine-learning` for convenience. It is an extension of `SVC` that takes in a quantum kernel instead of the `kernel.evaluate` method shown before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "\n",
    "qsvc = QSVC(quantum_kernel=adhoc_kernel)\n",
    "\n",
    "qsvc.fit(train_features, train_labels)\n",
    "\n",
    "qsvc_score = qsvc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"QSVC classification test score: {qsvc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Evaluation of models used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classification Model                    | Accuracy Score\")\n",
    "print(f\"---------------------------------------------------------\")\n",
    "print(f\"SVC using kernel as a callable function | {adhoc_score_callable_function:10.2f}\")\n",
    "print(f\"SVC using precomputed kernel matrix     | {adhoc_score_precomputed_kernel:10.2f}\")\n",
    "print(f\"QSVC                                    | {qsvc_score:10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the classification dataset is small, we find that the three models achieve 100% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kernel Principal Component Analysis\n",
    "\n",
    "This section focuses on a Principal Component Analysis task using a kernel PCA algorithm. We calculate a kernel matrix using a `ZZFeatureMap` and show that this approach translates the original features into a new space, where axes are chosen along principal components. In this space the classification task can be performed with a simpler model rather than an SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Defining the dataset\n",
    "\n",
    "We again use the _ad hoc dataset_, but now generated with a higher gap of `0.6` (previous example: `0.3`) between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhoc_dimension = 2\n",
    "train_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n",
    "    training_size=25,\n",
    "    test_size=10,\n",
    "    n=adhoc_dimension,\n",
    "    gap=0.6,\n",
    "    plot_data=False,\n",
    "    one_hot=False,\n",
    "    include_sample_total=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the training and test datasets below. Our ultimate goal in this section is to construct new coordinates where the two classes can be linearly separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(train_features, train_labels, test_features, test_labels, adhoc_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Defining the Quantum Kernel\n",
    "\n",
    "We proceed with the same kernel setup as it was in the classification task, namely a `ZZFeatureMap` circuit as a feature map and an instance of `FidelityQuantumKernel`. You might notice that in this case we do not provide a `fidelity` instance. This is because the `ComputeUncompute` method provided in the previous case is instantiated by default when the fidelity instance is not provided explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = ZZFeatureMap(feature_dimension=2, reps=2, entanglement=\"linear\")\n",
    "#qpca_kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=feature_map)\n",
    "qpca_kernel = FidelityQuantumKernel(feature_map=feature_map) # ComputeUncompute method using built-in Sampler initialized by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we evaluate kernel matrices for the training and test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_matrix_train = qpca_kernel.evaluate(x_vec=train_features)\n",
    "k_matrix_test = qpca_kernel.evaluate(x_vec=test_features, y_vec=train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Comparison of Kernel PCA on gaussian and quantum kernel\n",
    "\n",
    "In this section we use the `KernelPCA` implementation from `scikit-learn`, with the `kernel` parameter set to \"rbf\" for a gaussian kernel and \"precomputed\" for a quantum kernel. The former is a very popular kernel function used in classical machine learning models, whereas the latter allows us to use a quantum kernel using its associated precomputed kernel matrix, which we have previously instantiated using `qpca_kernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "kernel_pca_rbf = KernelPCA(n_components=2, kernel=\"rbf\")\n",
    "kernel_pca_rbf.fit(train_features)\n",
    "train_features_rbf = kernel_pca_rbf.transform(train_features)\n",
    "test_features_rbf = kernel_pca_rbf.transform(test_features)\n",
    "\n",
    "kernel_pca_q = KernelPCA(n_components=2, kernel=\"precomputed\")\n",
    "kernel_pca_q.fit(k_matrix_train)\n",
    "train_features_q = kernel_pca_q.transform(k_matrix_train)\n",
    "test_features_q = kernel_pca_q.transform(k_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While usually PCA is used to reduce the number of features in a dataset, or in other words to reduce dimensionality of a dataset, we don't do that here. Instead, we keep the number of dimensions and employ the kernel PCA, mostly for visualization purposes, to show that classification on the transformed dataset becomes easily tractable by linear methods, like logistic regression. We use this method to separate two classes in the principal component space with a `LogisticRegression` model from `scikit-learn`. As usual we train it by calling the `fit` method on the training dataset and evaluate the model for accuracy with `score`.\n",
    "\n",
    "Here we train and score a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Using classical RBF kPCA preprocessing\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(train_features_rbf, train_labels)\n",
    "\n",
    "logistic_score = logistic_regression.score(train_features_rbf, train_labels)\n",
    "print(f\"Logistic regression (rbf kPCA preprocessed) train score: {logistic_score}\")\n",
    "logistic_score = logistic_regression.score(test_features_rbf, test_labels)\n",
    "print(f\"Logistic regression (rbf kPCA preprocessed) test score: {logistic_score}\")\n",
    "\n",
    "# Using Quantum kPCA preprocessing\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(train_features_q, train_labels)\n",
    "\n",
    "logistic_score = logistic_regression.score(train_features_q, train_labels)\n",
    "print(f\"Logistic regression (Quantum kPCA preprocessed) train score: {logistic_score}\")\n",
    "logistic_score = logistic_regression.score(test_features_q, test_labels)\n",
    "print(f\"Logistic regression (Quantum kPCA preprocessed) test score: {logistic_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can observe that the gaussian kernel based Kernel PCA model fails to make the dataset linearly separable, while the quantum kernel succeeds.\n",
    "\n",
    "Let's plot the results. First, we plot the transformed dataset we get with the quantum kernel. On the same plot we also add model results. Then, we plot the transformed dataset we get with the gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (rbf_ax, q_ax) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "plot_features(q_ax, train_features_q, train_labels, 0, \"s\", \"w\", \"b\", \"A train\")\n",
    "plot_features(q_ax, train_features_q, train_labels, 1, \"o\", \"w\", \"r\", \"B train\")\n",
    "plot_features(q_ax, test_features_q, test_labels, 0, \"s\", \"b\", \"w\", \"A test\")\n",
    "plot_features(q_ax, test_features_q, test_labels, 1, \"o\", \"r\", \"w\", \"A test\")\n",
    "\n",
    "q_ax.set_ylabel(\"Principal component #1\")\n",
    "q_ax.set_xlabel(\"Principal component #0\")\n",
    "q_ax.set_title(\"Projection of training and test data\\n using KPCA with Quantum Kernel\")\n",
    "\n",
    "# Plotting the linear separation\n",
    "h = 0.01  # step size in the mesh\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = train_features_q[:, 0].min() - 1, train_features_q[:, 0].max() + 1\n",
    "y_min, y_max = train_features_q[:, 1].min() - 1, train_features_q[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "predictions = logistic_regression.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "predictions = predictions.reshape(xx.shape)\n",
    "q_ax.contourf(xx, yy, predictions, cmap=plt.cm.RdBu, alpha=0.2)\n",
    "\n",
    "# Now classical RBF kernel\n",
    "plot_features(rbf_ax, train_features_rbf, train_labels, 0, \"s\", \"w\", \"b\", \"A train\")\n",
    "plot_features(rbf_ax, train_features_rbf, train_labels, 1, \"o\", \"w\", \"r\", \"B train\")\n",
    "plot_features(rbf_ax, test_features_rbf, test_labels, 0, \"s\", \"b\", \"w\", \"A test\")\n",
    "plot_features(rbf_ax, test_features_rbf, test_labels, 1, \"o\", \"r\", \"w\", \"A test\")\n",
    "\n",
    "rbf_ax.set_ylabel(\"Principal component #1\")\n",
    "rbf_ax.set_xlabel(\"Principal component #0\")\n",
    "#rbf_ax.set_title(\"Projection of training data\\n using KernelPCA\")\n",
    "rbf_ax.set_title(\"Projection of training data\\n using KernelPCA with classical rbf\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data points on the left figure are not linearly separable, but they are on the right one. Hence in case of quantum kernel we can apply linear models on the transformed dataset and this is why SVM classifier works perfectly well on the _ad hoc_ dataset as we saw in the [classification section](#2.-Classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusions\n",
    "\n",
    "In this tutorial:\n",
    "\n",
    "* We reviewed the fundamentals of quantum kernel learning\n",
    "* We understood how to define quantum kernels as instances of `FidelityQuantumKernel`\n",
    "* We learned how to use the `scikit-learn` `SVC` algorithm with a custom quantum kernel as a callable function vs precomputed quantum kernel matrix for classification\n",
    "* We learned how to train classifiers with the `QSVC` algorithm from `qiskit-machine-learning`\n",
    "* We investigated how to plug in a quantum kernel into `scikit-learn`'s `KernelPCA` algorithm and transform the ad-hoc dataset into a new one that can be tackled by a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further reference, `scikit-learn` has other algorithms that can use a precomputed kernel matrix, such as:\n",
    "\n",
    "- [Spectral clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html)\n",
    "- [Agglomerative clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
    "- [Support vector regression](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "- [Ridge regression](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html)\n",
    "- [Gaussian process regression](https://scikit-learn.org/stable/modules/gaussian_process.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exercises\n",
    "\n",
    "Now it your turn.\n",
    "\n",
    "Up to now all experiments were based on data extracted from the custom ad-hoc dataset. We will consider now a more realistic experimental scenario for which your tasks will consist of:\n",
    "\n",
    "1. Pick one reference classification dataset other than ad-hoc. You might consider one of the several built-in possibilities included within [Scikit-learn](https://scikit-learn.org/stable/datasets.html) or download it from other well-known public repositories such as [UCI](https://archive.ics.uci.edu/datasets) or [Kaggle](https://www.kaggle.com/datasets). Split the chosen dataset into train and test partitions with respective 80%-20% propotion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in here with your code and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Rearrange the dataset by selecting 2 out of the $n$ original features and plot the resulting data. Train both classical and quantum SVM models using the training data. Use an RBF kernel for the classical model and let default parameterization for $sigma$ and $C$. Use the same ZZFeatureMap described above in the case of the quantum kernel. Calculate and plot the associated kernel matrices, and report the corresponding performance in both the training and test partitions. Discuss and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in here with your code and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using the same 2-feature dataset train now a classical logistic regression model and report the associated train and test performances. Apply now kernel PCA to transform the data into new features as seen in the tutorial above, and train two additional logistic regression models, respectively again, to compare classical RBF and quantum ZZFeatureMap-based feature mappings. Report achieved performances on the training and test partitions for each of the three resulting scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in here with your code and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Taking now the initial $n$-feature input space, use kPCA for the purpose of dimensionality reduction using the first 2 principal components of the data. With the new resulting training and testing partitions, train a plain logistic regression model and report on the new obtained performances. Do it twice, separatedly using both classical RBF- and quantum-based kernels for the kPCA step. Compare the results with those achieved in the previous two exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in here with your code and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. Now experiment a bit yourself with the objective to obtain the best possible performance in the testing dataset. Take as reference any models you like. You might try increasing the input dimensionality (up to the original $n$-feature space, but be careful, depending on the chosen dataset this might involve a big penalty on the computational cost), or experiment with different hyper-paramterization for both classical or quantum-based learning models (for example, try additional classical kernels, e.g. linear, polynomial, ..., C-regularization, or different quantum-based feature mappings). Discuss the results and state your final conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in here with your code and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Delivery instructions\n",
    "\n",
    "- Work can be done in groups up to two persons\n",
    "\n",
    "- Deadline is December 31st, 2023"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "qiskit_latest",
   "language": "python",
   "name": "qiskit_latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
